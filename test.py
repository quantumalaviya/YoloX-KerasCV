# -*- coding: utf-8 -*-
"""KerasCV YoloX

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wrDpRI6f9SL8YWyhoO9SJvy-LjvIi6HE
"""
import tensorflow as tf
import tensorflow_datasets as tfds
from tensorflow import keras
from tensorflow.keras import optimizers

import keras_cv
from keras_cv import bounding_box
import os
from luketils import visualization
from __internal__.layers.yolox_label_encoder import YoloXLabelEncoder

BATCH_SIZE = 4
EPOCHS = int(os.getenv("EPOCHS", "10"))
CHECKPOINT_PATH = os.getenv("CHECKPOINT_PATH", "checkpoint/")
INFERENCE_CHECKPOINT_PATH = os.getenv("INFERENCE_CHECKPOINT_PATH", CHECKPOINT_PATH)

dataset, dataset_info = keras_cv.datasets.pascal_voc.load(
    split="train", bounding_box_format="xywh", batch_size=BATCH_SIZE
)

class_ids = [
    "Aeroplane",
    "Bicycle",
    "Bird",
    "Boat",
    "Bottle",
    "Bus",
    "Car",
    "Cat",
    "Chair",
    "Cow",
    "Dining Table",
    "Dog",
    "Horse",
    "Motorbike",
    "Person",
    "Potted Plant",
    "Sheep",
    "Sofa",
    "Train",
    "Tvmonitor",
    "Total",
]
class_mapping = dict(zip(range(len(class_ids)), class_ids))


def visualize_dataset(dataset, bounding_box_format):
    example = next(iter(dataset))
    images, boxes = example["images"], example["bounding_boxes"]
    visualization.plot_bounding_box_gallery(
        images,
        value_range=(0, 255),
        bounding_box_format=bounding_box_format,
        y_true=boxes,
        scale=4,
        rows=3,
        cols=3,
        show=True,
        thickness=4,
        font_scale=1,
        class_mapping=class_mapping,
    )


# visualize_dataset(dataset, bounding_box_format="xywh")

train_ds, train_dataset_info = keras_cv.datasets.pascal_voc.load(
    bounding_box_format="xywh", split="train", batch_size=BATCH_SIZE
)
val_ds, val_dataset_info = keras_cv.datasets.pascal_voc.load(
    bounding_box_format="xywh", split="validation", batch_size=BATCH_SIZE
)

random_flip = keras_cv.layers.RandomFlip(mode="horizontal", bounding_box_format="xywh")
rand_augment = keras_cv.layers.RandAugment(
    value_range=(0, 255),
    augmentations_per_image=2,
    # we disable geometric augmentations for object detection tasks
    geometric=False,
)


def augment(inputs):
    # In future KerasCV releases, RandAugment will support
    # bounding box detection
    inputs["images"] = rand_augment(inputs["images"])
    inputs = random_flip(inputs)
    return inputs


train_ds = train_ds.map(augment, num_parallel_calls=tf.data.AUTOTUNE)
# visualize_dataset(train_ds, bounding_box_format="xywh")

def dict_to_tuple(inputs):
    return inputs["images"], inputs["bounding_boxes"]


train_ds = train_ds.map(dict_to_tuple, num_parallel_calls=tf.data.AUTOTUNE)
val_ds = val_ds.map(dict_to_tuple, num_parallel_calls=tf.data.AUTOTUNE)

train_ds = train_ds.prefetch(tf.data.AUTOTUNE)
val_ds = val_ds.prefetch(tf.data.AUTOTUNE)

import tensorflow as tf

from keras_cv import bounding_box


class GIoULoss(tf.keras.losses.Loss):
    """Implements the GIoU Loss

    GIoU loss is a modified IoU loss commonly used for object detection. This loss aims
    to directly optimize the IoU score between true boxes and predicted boxes. GIoU loss
    adds a penalty term to the IoU loss that takes in account the area of the
    smallest box enclosing both the boxes being considered for the iou. The length of
    the last dimension should be atleast 4 to represent the bounding boxes. While
    this dimension can have more than 4 values, these values will be ignored for the
    calculation of this loss.

    Args:
        bounding_box_format: a case-insensitive string (for example, "xyxy").
            Each bounding box is defined by at least these 4 values. The inputs
            may contain additional information such as classes and confidence after
            these 4 values but these values will be ignored while calculating
            this loss. For detailed information on the supported formats, see the
            [KerasCV bounding box documentation](https://keras.io/api/keras_cv/bounding_box/formats/).

    References:
        - [GIoU paper](https://arxiv.org/pdf/1902.09630)
        - [TFAddons Implementation](https://www.tensorflow.org/addons/api_docs/python/tfa/losses/GIoULoss)

    Sample Usage:
    ```python
    y_true = tf.random.uniform((5, 10, 5), minval=0, maxval=10, dtype=tf.dtypes.int32)
    y_pred = tf.random.uniform((5, 10, 4), minval=0, maxval=10, dtype=tf.dtypes.int32)
    loss = GIoULoss(bounding_box_format = "xyWH")
    loss(y_true, y_pred).numpy()
    ```
    Usage with the `compile()` API:
    ```python
    model.compile(optimizer='adam', loss=keras_cv.losses.GIoULoss())
    ```
    """

    def __init__(self, bounding_box_format, **kwargs):
        super().__init__(**kwargs)
        self.bounding_box_format = bounding_box_format

    def _compute_giou(self, boxes1, boxes2):
        boxes1_rank = len(boxes1.shape)
        boxes2_rank = len(boxes2.shape)

        if (
            boxes1_rank != boxes2_rank
            or boxes1_rank not in [2, 3]
            or boxes2_rank not in [2, 3]
        ):
            raise ValueError(
                "`GIoULoss` expects both boxes to be batched, or both "
                f"boxes to be unbatched.  Received `len(boxes1.shape)`={boxes1_rank}, "
                f"`len(boxes2.shape)`={boxes2_rank}.  Expected either `len(boxes1.shape)`=2 AND "
                "`len(boxes2.shape)`=2, or `len(boxes1.shape)`=3 AND `len(boxes2.shape)`=3."
            )

        if self.bounding_box_format.startswith("rel"):
            target = "rel_yxyx"
        else:
            target = "yxyx"

        boxes1 = bounding_box.convert_format(
            boxes1, source=self.bounding_box_format, target=target
        )

        boxes2 = bounding_box.convert_format(
            boxes2, source=self.bounding_box_format, target=target
        )

        def compute_giou_for_batch(boxes):
            boxes1, boxes2 = boxes
            zero = tf.convert_to_tensor(0.0, boxes1.dtype)
            boxes1_ymin, boxes1_xmin, boxes1_ymax, boxes1_xmax = tf.unstack(
                boxes1[..., :4, None], 4, axis=-2
            )
            boxes2_ymin, boxes2_xmin, boxes2_ymax, boxes2_xmax = tf.unstack(
                boxes2[None, ..., :4], 4, axis=-1
            )
            boxes1_width = tf.maximum(zero, boxes1_xmax - boxes1_xmin)
            boxes1_height = tf.maximum(zero, boxes1_ymax - boxes1_ymin)
            boxes2_width = tf.maximum(zero, boxes2_xmax - boxes2_xmin)
            boxes2_height = tf.maximum(zero, boxes2_ymax - boxes2_ymin)
            boxes1_area = boxes1_width * boxes1_height
            boxes2_area = boxes2_width * boxes2_height
            intersect_ymin = tf.maximum(boxes1_ymin, boxes2_ymin)
            intersect_xmin = tf.maximum(boxes1_xmin, boxes2_xmin)
            intersect_ymax = tf.minimum(boxes1_ymax, boxes2_ymax)
            intersect_xmax = tf.minimum(boxes1_xmax, boxes2_xmax)
            intersect_width = tf.maximum(zero, intersect_xmax - intersect_xmin)
            intersect_height = tf.maximum(zero, intersect_ymax - intersect_ymin)
            intersect_area = intersect_width * intersect_height

            union_area = boxes1_area + boxes2_area - intersect_area
            iou = tf.math.divide_no_nan(intersect_area, union_area)

            # giou calculation
            enclose_ymin = tf.minimum(boxes1_ymin, boxes2_ymin)
            enclose_xmin = tf.minimum(boxes1_xmin, boxes2_xmin)
            enclose_ymax = tf.maximum(boxes1_ymax, boxes2_ymax)
            enclose_xmax = tf.maximum(boxes1_xmax, boxes2_xmax)
            enclose_width = tf.maximum(zero, enclose_xmax - enclose_xmin)
            enclose_height = tf.maximum(zero, enclose_ymax - enclose_ymin)
            enclose_area = enclose_width * enclose_height
            return iou - tf.math.divide_no_nan(
                (enclose_area - union_area), enclose_area
            )

        if boxes1_rank == 2:
            return compute_giou_for_batch((boxes1, boxes2))
        else:
            return tf.vectorized_map(compute_giou_for_batch, elems=(boxes1, boxes2))

    def call(self, y_true, y_pred):
        y_pred = tf.convert_to_tensor(y_pred)
        y_true = tf.cast(y_true, y_pred.dtype)

        giou = self._compute_giou(y_true, y_pred)
        giou = tf.linalg.diag_part(giou)
        # mean_giou = tf.reduce_mean(giou, axis=[-2, -1])

        return 1 - giou

    def get_config(self):
        config = super().get_config()
        config.update(
            {
                "bounding_box_format": self.bounding_box_format,
            }
        )
        return config

from yolox import YoloX

import keras_cv
import numpy as np
import tensorflow as tf
import tensorflow.keras.backend as K
from keras_cv import bounding_box
from keras_cv.models.object_detection.object_detection_base_model import \
    ObjectDetectionBaseModel
from tensorflow import keras


model = YoloX(
    # number of classes to be used in box classification
    classes=20,
    # For more info on supported bounding box formats, visit
    # https://keras.io/api/keras_cv/bounding_box/
    bounding_box_format="xywh",
    phi="s",
    # KerasCV offers a set of pre-configured backbones
    backbone="cspdarknet",
    # Each backbone comes with multiple pre-trained weights
    # These weights match the weights available in the `keras_cv.model` class.
    # include_rescaling tells the model whether your input images are in the default
    # pixel range (0, 255) or if you have already rescaled your inputs to the range
    # (0, 1).  In our case, we feed our model images with inputs in the range (0, 255).
    include_rescaling=False,
    # Typically, you'll want to set this to False when training a real model.
    # evaluate_train_time_metrics=True makes `train_step()` incompatible with TPU,
    # and also causes a massive performance hit.  It can, however be useful to produce
    # train time metrics when debugging your model training pipeline.
    evaluate_train_time_metrics=False,
)

metrics = [
    keras_cv.metrics.COCOMeanAveragePrecision(
        class_ids=range(20),
        bounding_box_format="xywh",
        name="Mean Average Precision",
    ),
    keras_cv.metrics.COCORecall(
        class_ids=range(20),
        bounding_box_format="xywh",
        max_detections=100,
        name="Recall",
    ),
]

optimizer = tf.optimizers.SGD()
model.compile(
    classification_loss=tf.keras.losses.BinaryCrossentropy(from_logits=True, reduction="none"),
    objectness_loss=tf.keras.losses.BinaryCrossentropy(from_logits=True, reduction="none"),
    box_loss=GIoULoss(bounding_box_format="center_xywh", reduction="none"),
    optimizer=optimizer,
    metrics=[
        keras_cv.metrics.COCOMeanAveragePrecision(
            class_ids=range(20),
            bounding_box_format="xywh",
            name="Mean Average Precision",
        ),
        keras_cv.metrics.COCORecall(
            class_ids=range(20),
            bounding_box_format="xywh",
            max_detections=100,
            name="Recall",
        ),
    ],
)

callbacks = [
    keras.callbacks.TensorBoard(log_dir="logs"),
    keras.callbacks.ReduceLROnPlateau(patience=5),
    # Uncomment to train your own RetinaNet
    keras.callbacks.ModelCheckpoint(CHECKPOINT_PATH, save_weights_only=True),
]

# model.load_weights(INFERENCE_CHECKPOINT_PATH)

model.fit(
    train_ds,
    validation_data=val_ds.take(20),
    epochs=EPOCHS,
    callbacks=callbacks,
)
model.save_weights(CHECKPOINT_PATH)

model.load_weights(INFERENCE_CHECKPOINT_PATH)
def visualize_detections(model, bounding_box_format):
    train_ds, val_dataset_info = keras_cv.datasets.pascal_voc.load(
        bounding_box_format=bounding_box_format, split="train", batch_size=BATCH_SIZE
    )
    train_ds = train_ds.map(dict_to_tuple, num_parallel_calls=tf.data.AUTOTUNE)
    images, y_true = next(iter(train_ds.take(1)))
    y_pred = model.predict(images)
    visualization.plot_bounding_box_gallery(
        images,
        value_range=(0, 255),
        bounding_box_format=bounding_box_format,
        y_true=y_true,
        y_pred=y_pred,
        scale=4,
        rows=2,
        cols=2,
        show=True,
        thickness=4,
        font_scale=1,
        class_mapping=class_mapping,
    )

visualize_detections(model, bounding_box_format="xywh")
